# -*- coding: utf-8 -*-
"""
nxs_app_turbo.py â€” NXS â€¢ AirportOps AI (Stable Turbo Edition)
-------------------------------------------------------------
Ù‡Ø°Ù‡ Ø§Ù„Ù†Ø³Ø®Ø© Ù…ØµÙ…Ù…Ø© Ù„ØªÙƒÙˆÙ†:
- Ø³Ø±ÙŠØ¹Ø© âš¡
- Ù…Ø³ØªÙ‚Ø±Ø© ðŸ›¡ï¸ (Ù„Ø§ ØªØ±Ø¬Ø¹ 500 Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…ØŒ Ø¨Ù„ Ø±Ø³Ø§Ù„Ø© Ù…ÙÙ‡ÙˆÙ…Ø© Ø¯Ø§Ø¦Ù…Ø§Ù‹)
- Ù…ØªÙˆØ§ÙÙ‚Ø© Ù…Ø¹ nxs_brain (Ø£ÙŠ Ù†Ø³Ø®Ø© Ø­Ø§Ù„ÙŠØ© Ù„Ø¯ÙŠÙƒ)
- Ø¨Ø¯ÙˆÙ† Ø£ÙŠ Ø°ÙƒØ± Ù…Ø¨Ø§Ø´Ø± Ù„Ù…Ø­Ø±Ùƒ Gemini ÙÙŠ Ø§Ù„Ù€ API

Ù…Ù„Ø§Ø­Ø¸Ø©:
- Ø¥Ø°Ø§ Ø­Ø¯Ø« Ø£ÙŠ Ø®Ø·Ø£ Ø¯Ø§Ø®Ù„ nxs_brainØŒ Ø³ÙŠØªÙ… Ø§Ù„ØªÙ‚Ø§Ø·Ù‡ ÙˆØ¥Ø±Ø¬Ø§Ø¹ Ø±Ø³Ø§Ù„Ø© Ù†ØµÙŠØ© Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù…Ø¹ meta ØªÙˆØ¶Ø­ Ø§Ù„Ø®Ø·Ø£.
- Ù„Ø§ Ù†Ø³ØªØ®Ø¯Ù… HTTPException Ø¨Ø±Ù…Ø¬ÙŠØ© 500 Ø­ØªÙ‰ Ù„Ø§ ØªØ¸Ù‡Ø± Ø±Ø³Ø§Ù„Ø© "Error: empty reply from server" ÙÙŠ Ø§Ù„ÙˆØ§Ø¬Ù‡Ø©.
"""

import time
import logging
from typing import Optional, Dict, Any

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

from nxs_brain import nxs_brain

app = FastAPI(
    title="NXS â€¢ AirportOps AI",
    version="2.1-stable-turbo",
)

# ---------------- Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø³Ø¬Ù„ ----------------
logger = logging.getLogger("nxs_app_turbo")

# ---------------- CORS ----------------
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
    allow_credentials=True,
)


# ------------- Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø·Ù„Ø¨ / Ø§Ù„Ø±Ø¯ -------------
class ChatRequest(BaseModel):
    message: str
    lang: Optional[str] = "ar"  # Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ù„Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ Ø¥Ø°Ø§ Ø£Ø­Ø¨Ø¨Ù†Ø§ ØªÙ…Ø±ÙŠØ± Ø§Ù„Ù„ØºØ© Ù…Ù† Ø§Ù„ÙˆØ§Ø¬Ù‡Ø©


class ChatResponse(BaseModel):
    reply: str
    meta: Optional[Dict[str, Any]] = None
    latency_ms: Optional[float] = None


# ------------- ÙƒØ§Ø´ Ø¨Ø³ÙŠØ· Ù„ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…ÙƒØ±Ø±Ø© -------------
CACHE: Dict[str, Dict[str, Any]] = {}
CACHE_TTL = 20  # Ø«ÙˆØ§Ù†Ù Ù‚Ù„ÙŠÙ„Ø© Ù„Ø¹Ù…Ø± Ø§Ù„ÙƒØ§Ø´ (Ù‚ØµÙŠØ± Ø­ØªÙ‰ Ù†Ø¨Ù‚Ù‰ Ø£Ù‚Ø±Ø¨ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­ÙŠØ©)


def cache_get(key: str) -> Optional[Dict[str, Any]]:
    item = CACHE.get(key)
    if not item:
        return None
    if time.time() - item["time"] > CACHE_TTL:
        return None
    return item["value"]


def cache_set(key: str, value: Dict[str, Any]) -> None:
    CACHE[key] = {"value": value, "time": time.time()}


# ------------- Ù†Ù‚Ø·Ø© /chat Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© -------------
@app.post("/chat", response_model=ChatResponse)
async def chat(req: ChatRequest) -> ChatResponse:
    """
    Ù†Ù‚Ø·Ø© Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©:
    - ØªÙ‚Ø±Ø£ message Ù…Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù….
    - ØªÙ†Ø¸Ù‘ÙÙ‡Ø§ ÙˆØªØªØ£ÙƒØ¯ Ù…Ù† Ø¹Ø¯Ù… ÙƒÙˆÙ†Ù‡Ø§ ÙØ§Ø±ØºØ©.
    - ØªØ³ØªØ¯Ø¹ÙŠ nxs_brain(message).
    - ØªØ±Ø¬Ø¹ Ø§Ù„Ø±Ø¯ + meta + Ø²Ù…Ù† Ø§Ù„ØªÙ†ÙÙŠØ°.
    - ÙÙŠ Ø­Ø§Ù„Ø© Ø£ÙŠ Ø®Ø·Ø£ØŒ ØªØ±Ø¬Ø¹ Ø±Ø³Ø§Ù„Ø© Ù†ØµÙŠØ© Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…ØŒ ÙˆÙ„ÙŠØ³ 500.
    """
    start = time.time()
    msg = (req.message or "").strip()

    if not msg:
        return ChatResponse(
            reply="Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„ÙˆØ§Ø±Ø¯Ø© ÙØ§Ø±ØºØ©. ÙŠØ±Ø¬Ù‰ ÙƒØªØ§Ø¨Ø© Ø³Ø¤Ø§Ù„Ùƒ Ø£Ùˆ Ø·Ù„Ø¨Ùƒ Ø¨Ø´ÙƒÙ„ ÙˆØ§Ø¶Ø­.",
            meta={"ok": False, "reason": "empty_message"},
            latency_ms=round((time.time() - start) * 1000.0, 2),
        )

    # 1) ÙØ­Øµ Ø§Ù„ÙƒØ§Ø´ (Ø¥Ø°Ø§ Ù†ÙØ³ Ø§Ù„Ø³Ø¤Ø§Ù„ ØªÙƒØ±Ø± Ø®Ù„Ø§Ù„ Ø§Ù„ÙØªØ±Ø© Ø§Ù„Ù‚ØµÙŠØ±Ø©)
    cached = cache_get(msg)
    if cached is not None:
        return ChatResponse(
            reply=cached["reply"],
            meta={
                **(cached.get("meta") or {}),
                "from_cache": True,
            },
            latency_ms=0.5,  # Ù„Ø£Ù† Ø§Ù„Ø±Ø¯ Ù…Ù† Ø§Ù„ÙƒØ§Ø´ Ø´Ø¨Ù‡ ÙÙˆØ±ÙŠ
        )

    # 2) Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ nxs_brain Ù…Ø¹ Ø­Ù…Ø§ÙŠØ© ÙƒØ§Ù…Ù„Ø© Ù…Ù† Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
    try:
        reply, meta = nxs_brain(msg)
        # Ø­ÙØ¸ ÙÙŠ Ø§Ù„ÙƒØ§Ø´
        cache_set(msg, {"reply": reply, "meta": meta})
        latency = round((time.time() - start) * 1000.0, 2)
        return ChatResponse(
            reply=reply,
            meta={
                **(meta or {}),
                "from_cache": False,
            },
            latency_ms=latency,
        )

    except Exception as e:  # pragma: no cover - Ø­Ù…Ø§ÙŠØ© Ø¯ÙØ§Ø¹ÙŠØ©
        # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠ
        logger.error("Unhandled error in /chat handler: %s", e, exc_info=True)

        fallback_reply = (
            "Ø­Ø¯Ø« Ø®Ø·Ø£ Ø¯Ø§Ø®Ù„ÙŠ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø·Ù„Ø¨ Ø¯Ø§Ø®Ù„ NXS â€¢ AirportOps AI. "
            "ÙŠÙ…ÙƒÙ† Ù…Ø±Ø§Ø¬Ø¹Ø© Ø³Ø¬Ù„ Ø§Ù„Ø®Ø§Ø¯Ù… (logs) Ù„Ù…Ø¹Ø±ÙØ© ØªÙØ§ØµÙŠÙ„ Ø£ÙƒØ«Ø± Ø¹Ù† Ø§Ù„Ø®Ø·Ø£."
        )
        latency = round((time.time() - start) * 1000.0, 2)
        return ChatResponse(
            reply=fallback_reply,
            meta={
                "ok": False,
                "error": str(e),
                "source": "nxs_app_turbo_chat_handler",
            },
            latency_ms=latency,
        )


# ------------- Ù†Ù‚Ø§Ø· ÙØ­Øµ Ø§Ù„ØµØ­Ø© / Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¹Ø§Ù…Ø© -------------
@app.get("/")
async def home() -> Dict[str, Any]:
    return {
        "status": "running",
        "engine": "NXS â€¢ AirportOps AI",
        "mode": "Stable Turbo",
        "version": "2.1-stable-turbo",
    }


@app.get("/health")
async def health() -> Dict[str, Any]:
    return {
        "status": "ok",
        "engine": "NXS â€¢ AirportOps AI",
        "uptime_mode": "Stable Turbo",
    }
